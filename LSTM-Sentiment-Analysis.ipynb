{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pipeline](pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM for Sentiment Analysis of Amazon Reviews\n",
    "\n",
    "Reference: https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 800000 #3600000\n",
    "TEST_SIZE = 200000 #40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_list(fileName, size):\n",
    "    lines = []\n",
    "    with open(fileName) as file:\n",
    "        for i in range(size):\n",
    "            lines.append(file.readline())\n",
    "    return lines\n",
    "\n",
    "train_set = file_to_list('train.ft.txt', TRAIN_SIZE)\n",
    "test_set = file_to_list('test.ft.txt', TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'__label__2 Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^\\n'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the training set...\n",
    "train_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate sentence from label\n",
    "\n",
    "\\__label__2 <review>     <--- Positive Sentiment\n",
    "                             \n",
    "\\__label__1 <review>     <--- Negative Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dataset): #Returns: (sentence,label)\n",
    "    label = []\n",
    "    sentences = []\n",
    "    positive = 0\n",
    "    negative = 0\n",
    "    for line in dataset:\n",
    "        sentences.append(line[11:])\n",
    "        if line[9] == '2':\n",
    "            positive += 1\n",
    "            label.append(1) # Positive Sentiment\n",
    "        else:\n",
    "            negative += 1   # Negative Sentiment\n",
    "            label.append(0)\n",
    "    return sentences, label, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labelling Training Set...\n",
      "Found 404094+ sentiments, 395906- sentiments in Training Set\n",
      "Labelling Test Set...\n",
      "Found 100565+ sentiments, 99435- sentiments in Test Set\n"
     ]
    }
   ],
   "source": [
    "print('Labelling Training Set...')\n",
    "train_sentences, train_label, positive, negative = extract_data(train_set)\n",
    "print(f'Found {positive}+ sentiments, {negative}- sentiments in Training Set')\n",
    "\n",
    "print('Labelling Test Set...')\n",
    "test_sentences, test_label, positive, negative = extract_data(test_set)\n",
    "print(f'Found {positive}+ sentiments, {negative}- sentiments in Test Set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fb230e390a4b56bc03d0a128cabd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c539d0e6d7f546a481ee194e70479ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "# replace \\d with 0 in the vector\n",
    "# Modify URLS to <url>\n",
    "import re\n",
    "from tqdm.autonotebook import tqdm # Progress Bar\n",
    "for i in tqdm(range(len(train_sentences))):\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n",
    "        \n",
    "for i in tqdm(range(len(test_sentences))):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Training Sentences: 800000\n",
      "Valid Test Sentences: 200000\n"
     ]
    }
   ],
   "source": [
    "# Remove empty strings\n",
    "train_sentences = list(filter(None, train_sentences))\n",
    "test_sentences = list(filter(None, test_sentences))\n",
    "print(f'Valid Training Sentences: {len(train_sentences)}')\n",
    "print(f'Valid Test Sentences: {len(test_sentences)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Use ntlk.regexp_tokenize for faster tokenization over ntlk.word_tokenize. Regexp tokenizer is around 6x faster\n",
    "\n",
    "Reference: https://towardsdatascience.com/benchmarking-python-nlp-tokenizers-3ac4735100c5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ryzen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e665e646263543609ac8b4d724812383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "100% done\n"
     ]
    }
   ],
   "source": [
    "# Word to Frequency\n",
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt') # Tokenizer\n",
    "words = Counter()\n",
    "for i, sentence in enumerate(tqdm(train_sentences)):\n",
    "    try:\n",
    "        #tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = nltk.regexp_tokenize(sentence, pattern=\"\\s+\", gaps = True)\n",
    "        train_sentences[i] = []\n",
    "        for word in tokens: # Tokenize the words\n",
    "            words.update([word.lower()]) # To Lower Case\n",
    "            train_sentences[i].append(word)\n",
    "    except:\n",
    "        print(sentence)\n",
    "print(\"100% done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove infrequent words (i.e. words that only appear once)\n",
    "words = {k:v for k,v in words.items() if v>1}\n",
    "# Sort the words according to frequency, descending order\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "# Add padding & unknown to corpus\n",
    "words = ['_PAD','_UNK'] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries for fast mappings\n",
    "word2idx = {w:i for i,w in enumerate(words)}\n",
    "idx2word = {i:w for i,w in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360d1e339f2d4309860b306ea207130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0528e1288d7448a7971c513da69dd85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=200000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert word to indices\n",
    "for i, sentence in enumerate(tqdm(train_sentences)):\n",
    "    train_sentences[i] = [word2idx[word] if word in word2idx else 0 for word in sentence]\n",
    "    \n",
    "for i, sentence in enumerate(tqdm(test_sentences)):\n",
    "    # For test sentences, we have to tokenize the sentences as well\n",
    "    test_sentences[i] = [word2idx[word.lower()] if word in word2idx else 0 for word in nltk.regexp_tokenize(sentence, pattern=\"\\s+\", gaps = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save clean data\n",
    "import pickle\n",
    "with open('train_sentences.data', 'wb') as file:\n",
    "    pickle.dump(train_sentences, file)\n",
    "with open('test_sentences.data', 'wb') as file:\n",
    "    pickle.dump(test_sentences, file)\n",
    "with open('train_label.data', 'wb') as file:\n",
    "    pickle.dump(train_label, file)\n",
    "with open('test_label.data', 'wb') as file:\n",
    "    pickle.dump(test_label, file)\n",
    "with open('words.data', 'wb') as file:\n",
    "    pickle.dump(words, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "from tqdm.autonotebook import tqdm\n",
    "import pickle\n",
    "import nltk\n",
    "with open('train_sentences.data', 'rb') as file:\n",
    "    train_sentences = pickle.load(file)\n",
    "with open('test_sentences.data', 'rb') as file:\n",
    "    test_sentences = pickle.load(file)\n",
    "with open('train_label.data', 'rb') as file:\n",
    "    train_label = pickle.load(file)\n",
    "with open('test_label.data', 'rb') as file:\n",
    "    test_label = pickle.load(file)\n",
    "with open('words.data', 'rb') as file:\n",
    "    words = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,     70,     12,      2,      0,      0,    196,    500,\n",
       "           14,  14240,      0,   6581,      2, 192228,     11,     59,\n",
       "          524,     27,    103,      0,     41,   1618,     10,     70,\n",
       "            6,    127,     66,    574,  80188,    233,   6188,      0,\n",
       "           20,    498,      2,    233,      0,      0,     17,     48,\n",
       "            7,     31,      7,      2,   1015,      0,     20,    128,\n",
       "          498,     10,     42,      2,     90,   6188,      0,  10459,\n",
       "          307,     35,   6494, 135417,      3,    372,      4,  28357,\n",
       "         1111,     18,  11879,   3973,      3,   5310,  80189,      0,\n",
       "           41,   5711,    199,     66,   3726,      6,  29906,  33086])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2989c1bca7cc437f9a3e56bd666c089f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=800000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c03ebae31448cd9da02e69c009eaa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=100000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Padding\n",
    "import numpy as np\n",
    "def pad_input(sentences, seq_len):\n",
    "    features = np.zeros((len(sentences), seq_len), dtype=int)\n",
    "    for i, review in enumerate(tqdm(sentences)):\n",
    "        if len(review) != 0:\n",
    "            features[i, -len(review):] = np.array(review)[:seq_len]\n",
    "    return features\n",
    "\n",
    "SEQUENCE_LENGTH = 200\n",
    "train_sentences = pad_input(train_sentences, SEQUENCE_LENGTH)\n",
    "test_sentences = pad_input(test_sentences, SEQUENCE_LENGTH)\n",
    "\n",
    "# Label to numpy array\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,     70,     12,      2,      0,      0,    196,    500,\n",
       "           14,  14240,      0,   6581,      2, 192228,     11,     59,\n",
       "          524,     27,    103,      0,     41,   1618,     10,     70,\n",
       "            6,    127,     66,    574,  80188,    233,   6188,      0,\n",
       "           20,    498,      2,    233,      0,      0,     17,     48,\n",
       "            7,     31,      7,      2,   1015,      0,     20,    128,\n",
       "          498,     10,     42,      2,     90,   6188,      0,  10459,\n",
       "          307,     35,   6494, 135417,      3,    372,      4,  28357,\n",
       "         1111,     18,  11879,   3973,      3,   5310,  80189,      0,\n",
       "           41,   5711,    199,     66,   3726,      6,  29906,  33086])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_VALIDATION_SPLIT = 0.5\n",
    "split_index = int(TEST_VALIDATION_SPLIT * len(test_sentences))\n",
    "val_sentences, test_sentences = test_sentences[:split_index], test_sentences[split_index:]\n",
    "val_label, test_label = test_label[:split_index], test_label[split_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.from_numpy(train_sentences),\n",
    "    torch.from_numpy(train_label))\n",
    "test_data = TensorDataset(\n",
    "    torch.from_numpy(test_sentences),\n",
    "    torch.from_numpy(test_label))\n",
    "val_data = TensorDataset(\n",
    "    torch.from_numpy(val_sentences),\n",
    "    torch.from_numpy(val_label))\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GPU (CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda in training models.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {device} in training models.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(NLP, self).__init__()\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Layers:\n",
    "        # Embedding -> LSTM (n_layers times) -> Fully Connected\n",
    "        \n",
    "        # Create Embeddings\n",
    "        # Word to Vector\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # Define LSTM Model\n",
    "        # nn.LSTM(input, hidden, num_hidden_layers, dropout, batch_first=True)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        # Dropout (Deactivate some neurons randomly)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # Define the Fully Connected Layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        \n",
    "        # Activation Function\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0) # Rows\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
    "                 )\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 250\n",
    "hidden_dim = 350\n",
    "n_layers = 2\n",
    "\n",
    "model = NLP(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 100... Loss: 0.477714... Val Loss: 0.459629\n",
      "Validation loss decreased (inf --> 0.459629).  Saving model ...\n",
      "Epoch: 1/2... Step: 200... Loss: 0.315157... Val Loss: 0.337323\n",
      "Validation loss decreased (0.459629 --> 0.337323).  Saving model ...\n",
      "Epoch: 1/2... Step: 300... Loss: 0.360729... Val Loss: 0.307392\n",
      "Validation loss decreased (0.337323 --> 0.307392).  Saving model ...\n",
      "Epoch: 1/2... Step: 400... Loss: 0.354569... Val Loss: 0.291516\n",
      "Validation loss decreased (0.307392 --> 0.291516).  Saving model ...\n",
      "Epoch: 1/2... Step: 500... Loss: 0.327683... Val Loss: 0.278473\n",
      "Validation loss decreased (0.291516 --> 0.278473).  Saving model ...\n",
      "Epoch: 1/2... Step: 600... Loss: 0.287981... Val Loss: 0.281021\n",
      "Epoch: 1/2... Step: 700... Loss: 0.211321... Val Loss: 0.264161\n",
      "Validation loss decreased (0.278473 --> 0.264161).  Saving model ...\n",
      "Epoch: 1/2... Step: 800... Loss: 0.517902... Val Loss: 0.281967\n",
      "Epoch: 1/2... Step: 900... Loss: 0.225072... Val Loss: 0.257904\n",
      "Validation loss decreased (0.264161 --> 0.257904).  Saving model ...\n",
      "Epoch: 1/2... Step: 1000... Loss: 0.272186... Val Loss: 0.256639\n",
      "Validation loss decreased (0.257904 --> 0.256639).  Saving model ...\n",
      "Epoch: 1/2... Step: 1100... Loss: 0.286301... Val Loss: 0.250221\n",
      "Validation loss decreased (0.256639 --> 0.250221).  Saving model ...\n",
      "Epoch: 1/2... Step: 1200... Loss: 0.323884... Val Loss: 0.242013\n",
      "Validation loss decreased (0.250221 --> 0.242013).  Saving model ...\n",
      "Epoch: 1/2... Step: 1300... Loss: 0.225197... Val Loss: 0.234654\n",
      "Validation loss decreased (0.242013 --> 0.234654).  Saving model ...\n",
      "Epoch: 1/2... Step: 1400... Loss: 0.251142... Val Loss: 0.240384\n",
      "Epoch: 1/2... Step: 1500... Loss: 0.369322... Val Loss: 0.240738\n",
      "Epoch: 1/2... Step: 1600... Loss: 0.229167... Val Loss: 0.237578\n",
      "Epoch: 1/2... Step: 1700... Loss: 0.255402... Val Loss: 0.235367\n",
      "Epoch: 1/2... Step: 1800... Loss: 0.272631... Val Loss: 0.235786\n",
      "Epoch: 1/2... Step: 1900... Loss: 0.295676... Val Loss: 0.233252\n",
      "Validation loss decreased (0.234654 --> 0.233252).  Saving model ...\n",
      "Epoch: 1/2... Step: 2000... Loss: 0.252678... Val Loss: 0.231348\n",
      "Validation loss decreased (0.233252 --> 0.231348).  Saving model ...\n",
      "Epoch: 1/2... Step: 2100... Loss: 0.157330... Val Loss: 0.227215\n",
      "Validation loss decreased (0.231348 --> 0.227215).  Saving model ...\n",
      "Epoch: 1/2... Step: 2200... Loss: 0.300210... Val Loss: 0.228508\n",
      "Epoch: 1/2... Step: 2300... Loss: 0.247114... Val Loss: 0.225297\n",
      "Validation loss decreased (0.227215 --> 0.225297).  Saving model ...\n",
      "Epoch: 1/2... Step: 2400... Loss: 0.240871... Val Loss: 0.232893\n",
      "Epoch: 1/2... Step: 2500... Loss: 0.232353... Val Loss: 0.219194\n",
      "Validation loss decreased (0.225297 --> 0.219194).  Saving model ...\n",
      "Epoch: 1/2... Step: 2600... Loss: 0.299980... Val Loss: 0.222050\n",
      "Epoch: 1/2... Step: 2700... Loss: 0.328529... Val Loss: 0.220412\n",
      "Epoch: 1/2... Step: 2800... Loss: 0.290361... Val Loss: 0.242316\n",
      "Epoch: 1/2... Step: 2900... Loss: 0.278505... Val Loss: 0.221306\n",
      "Epoch: 1/2... Step: 3000... Loss: 0.363818... Val Loss: 0.220453\n",
      "Epoch: 1/2... Step: 3100... Loss: 0.226371... Val Loss: 0.216315\n",
      "Validation loss decreased (0.219194 --> 0.216315).  Saving model ...\n",
      "Epoch: 1/2... Step: 3200... Loss: 0.262631... Val Loss: 0.217384\n",
      "Epoch: 1/2... Step: 3300... Loss: 0.292732... Val Loss: 0.216088\n",
      "Validation loss decreased (0.216315 --> 0.216088).  Saving model ...\n",
      "Epoch: 1/2... Step: 3400... Loss: 0.317589... Val Loss: 0.214193\n",
      "Validation loss decreased (0.216088 --> 0.214193).  Saving model ...\n",
      "Epoch: 1/2... Step: 3500... Loss: 0.239118... Val Loss: 0.220177\n",
      "Epoch: 1/2... Step: 3600... Loss: 0.296302... Val Loss: 0.226331\n",
      "Epoch: 1/2... Step: 3700... Loss: 0.199903... Val Loss: 0.215385\n",
      "Epoch: 1/2... Step: 3800... Loss: 0.228385... Val Loss: 0.211732\n",
      "Validation loss decreased (0.214193 --> 0.211732).  Saving model ...\n",
      "Epoch: 1/2... Step: 3900... Loss: 0.186001... Val Loss: 0.214678\n",
      "Epoch: 1/2... Step: 4000... Loss: 0.239114... Val Loss: 0.217351\n",
      "Epoch: 1/2... Step: 4100... Loss: 0.337746... Val Loss: 0.217259\n",
      "Epoch: 1/2... Step: 4200... Loss: 0.428448... Val Loss: 0.218777\n",
      "Epoch: 1/2... Step: 4300... Loss: 0.265966... Val Loss: 0.212217\n",
      "Epoch: 1/2... Step: 4400... Loss: 0.249748... Val Loss: 0.214535\n",
      "Epoch: 1/2... Step: 4500... Loss: 0.251170... Val Loss: 0.211360\n",
      "Validation loss decreased (0.211732 --> 0.211360).  Saving model ...\n",
      "Epoch: 1/2... Step: 4600... Loss: 0.192820... Val Loss: 0.210618\n",
      "Validation loss decreased (0.211360 --> 0.210618).  Saving model ...\n",
      "Epoch: 1/2... Step: 4700... Loss: 0.249363... Val Loss: 0.210364\n",
      "Validation loss decreased (0.210618 --> 0.210364).  Saving model ...\n",
      "Epoch: 1/2... Step: 4800... Loss: 0.167022... Val Loss: 0.214439\n",
      "Epoch: 1/2... Step: 4900... Loss: 0.229949... Val Loss: 0.208314\n",
      "Validation loss decreased (0.210364 --> 0.208314).  Saving model ...\n",
      "Epoch: 1/2... Step: 5000... Loss: 0.246421... Val Loss: 0.212812\n",
      "Epoch: 1/2... Step: 5100... Loss: 0.288971... Val Loss: 0.212008\n",
      "Epoch: 1/2... Step: 5200... Loss: 0.242173... Val Loss: 0.217340\n",
      "Epoch: 1/2... Step: 5300... Loss: 0.198605... Val Loss: 0.210286\n",
      "Epoch: 1/2... Step: 5400... Loss: 0.367815... Val Loss: 0.209946\n",
      "Epoch: 1/2... Step: 5500... Loss: 0.222787... Val Loss: 0.224535\n",
      "Epoch: 1/2... Step: 5600... Loss: 0.386597... Val Loss: 0.217803\n",
      "Epoch: 1/2... Step: 5700... Loss: 0.286972... Val Loss: 0.217047\n",
      "Epoch: 1/2... Step: 5800... Loss: 0.301990... Val Loss: 0.217581\n",
      "Epoch: 1/2... Step: 5900... Loss: 0.174733... Val Loss: 0.215608\n",
      "Epoch: 1/2... Step: 6000... Loss: 0.312892... Val Loss: 0.224387\n",
      "Epoch: 1/2... Step: 6100... Loss: 0.220431... Val Loss: 0.219330\n",
      "Epoch: 1/2... Step: 6200... Loss: 0.256381... Val Loss: 0.212392\n",
      "Epoch: 1/2... Step: 6300... Loss: 0.359486... Val Loss: 0.213489\n",
      "Epoch: 1/2... Step: 6400... Loss: 0.296449... Val Loss: 0.217641\n",
      "Epoch: 1/2... Step: 6500... Loss: 0.291027... Val Loss: 0.216219\n",
      "Epoch: 1/2... Step: 6600... Loss: 0.294666... Val Loss: 0.215173\n",
      "Epoch: 1/2... Step: 6700... Loss: 0.290529... Val Loss: 0.212621\n",
      "Epoch: 1/2... Step: 6800... Loss: 0.206118... Val Loss: 0.215088\n",
      "Epoch: 1/2... Step: 6900... Loss: 0.285409... Val Loss: 0.212641\n",
      "Epoch: 1/2... Step: 7000... Loss: 0.300812... Val Loss: 0.222101\n",
      "Epoch: 1/2... Step: 7100... Loss: 0.316004... Val Loss: 0.223594\n",
      "Epoch: 1/2... Step: 7200... Loss: 0.274593... Val Loss: 0.218163\n",
      "Epoch: 1/2... Step: 7300... Loss: 0.283466... Val Loss: 0.237016\n",
      "Epoch: 1/2... Step: 7400... Loss: 0.236792... Val Loss: 0.228463\n",
      "Epoch: 1/2... Step: 7500... Loss: 0.278282... Val Loss: 0.218553\n",
      "Epoch: 1/2... Step: 7600... Loss: 0.223059... Val Loss: 0.226500\n",
      "Epoch: 1/2... Step: 7700... Loss: 0.145202... Val Loss: 0.224154\n",
      "Epoch: 1/2... Step: 7800... Loss: 0.348667... Val Loss: 0.211884\n",
      "Epoch: 1/2... Step: 7900... Loss: 0.261766... Val Loss: 0.218282\n",
      "Epoch: 1/2... Step: 8000... Loss: 0.247258... Val Loss: 0.219409\n",
      "Epoch: 2/2... Step: 8100... Loss: 0.213461... Val Loss: 0.214312\n",
      "Epoch: 2/2... Step: 8200... Loss: 0.341599... Val Loss: 0.212942\n",
      "Epoch: 2/2... Step: 8300... Loss: 0.226828... Val Loss: 0.217168\n",
      "Epoch: 2/2... Step: 8400... Loss: 0.215449... Val Loss: 0.216680\n",
      "Epoch: 2/2... Step: 8500... Loss: 0.224372... Val Loss: 0.218861\n",
      "Epoch: 2/2... Step: 8600... Loss: 0.258253... Val Loss: 0.231271\n",
      "Epoch: 2/2... Step: 8700... Loss: 0.263543... Val Loss: 0.230162\n",
      "Epoch: 2/2... Step: 8800... Loss: 0.329417... Val Loss: 0.222939\n",
      "Epoch: 2/2... Step: 8900... Loss: 0.155861... Val Loss: 0.220386\n",
      "Epoch: 2/2... Step: 9000... Loss: 0.164335... Val Loss: 0.220126\n",
      "Epoch: 2/2... Step: 9100... Loss: 0.205984... Val Loss: 0.219692\n",
      "Epoch: 2/2... Step: 9200... Loss: 0.285406... Val Loss: 0.214299\n",
      "Epoch: 2/2... Step: 9300... Loss: 0.181300... Val Loss: 0.220155\n",
      "Epoch: 2/2... Step: 9400... Loss: 0.185341... Val Loss: 0.218767\n",
      "Epoch: 2/2... Step: 9500... Loss: 0.263556... Val Loss: 0.228933\n",
      "Epoch: 2/2... Step: 9600... Loss: 0.263867... Val Loss: 0.216528\n",
      "Epoch: 2/2... Step: 9700... Loss: 0.228860... Val Loss: 0.217795\n",
      "Epoch: 2/2... Step: 9800... Loss: 0.307221... Val Loss: 0.217839\n",
      "Epoch: 2/2... Step: 9900... Loss: 0.229066... Val Loss: 0.225253\n",
      "Epoch: 2/2... Step: 10000... Loss: 0.199240... Val Loss: 0.227832\n",
      "Epoch: 2/2... Step: 10100... Loss: 0.299158... Val Loss: 0.221775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/2... Step: 10200... Loss: 0.201613... Val Loss: 0.219252\n",
      "Epoch: 2/2... Step: 10300... Loss: 0.249252... Val Loss: 0.215504\n",
      "Epoch: 2/2... Step: 10400... Loss: 0.190738... Val Loss: 0.216093\n",
      "Epoch: 2/2... Step: 10500... Loss: 0.237577... Val Loss: 0.217155\n",
      "Epoch: 2/2... Step: 10600... Loss: 0.243320... Val Loss: 0.233817\n",
      "Epoch: 2/2... Step: 10700... Loss: 0.265506... Val Loss: 0.217724\n",
      "Epoch: 2/2... Step: 10800... Loss: 0.309450... Val Loss: 0.219982\n",
      "Epoch: 2/2... Step: 10900... Loss: 0.239879... Val Loss: 0.223056\n",
      "Epoch: 2/2... Step: 11000... Loss: 0.347995... Val Loss: 0.217781\n",
      "Epoch: 2/2... Step: 11100... Loss: 0.167716... Val Loss: 0.226556\n",
      "Epoch: 2/2... Step: 11200... Loss: 0.238422... Val Loss: 0.219296\n",
      "Epoch: 2/2... Step: 11300... Loss: 0.208327... Val Loss: 0.225786\n",
      "Epoch: 2/2... Step: 11400... Loss: 0.263808... Val Loss: 0.225351\n",
      "Epoch: 2/2... Step: 11500... Loss: 0.243468... Val Loss: 0.217430\n",
      "Epoch: 2/2... Step: 11600... Loss: 0.246750... Val Loss: 0.222807\n",
      "Epoch: 2/2... Step: 11700... Loss: 0.234590... Val Loss: 0.227312\n",
      "Epoch: 2/2... Step: 11800... Loss: 0.209096... Val Loss: 0.220989\n",
      "Epoch: 2/2... Step: 11900... Loss: 0.209869... Val Loss: 0.221351\n",
      "Epoch: 2/2... Step: 12000... Loss: 0.262193... Val Loss: 0.224264\n",
      "Epoch: 2/2... Step: 12100... Loss: 0.237392... Val Loss: 0.219921\n",
      "Epoch: 2/2... Step: 12200... Loss: 0.152401... Val Loss: 0.223003\n",
      "Epoch: 2/2... Step: 12300... Loss: 0.191215... Val Loss: 0.225821\n",
      "Epoch: 2/2... Step: 12400... Loss: 0.213494... Val Loss: 0.222706\n",
      "Epoch: 2/2... Step: 12500... Loss: 0.116464... Val Loss: 0.229937\n",
      "Epoch: 2/2... Step: 12600... Loss: 0.404911... Val Loss: 0.230499\n",
      "Epoch: 2/2... Step: 12700... Loss: 0.222259... Val Loss: 0.230425\n",
      "Epoch: 2/2... Step: 12800... Loss: 0.276001... Val Loss: 0.223426\n",
      "Epoch: 2/2... Step: 12900... Loss: 0.281088... Val Loss: 0.224236\n",
      "Epoch: 2/2... Step: 13000... Loss: 0.179039... Val Loss: 0.227241\n",
      "Epoch: 2/2... Step: 13100... Loss: 0.200781... Val Loss: 0.218276\n",
      "Epoch: 2/2... Step: 13200... Loss: 0.224098... Val Loss: 0.217105\n",
      "Epoch: 2/2... Step: 13300... Loss: 0.221793... Val Loss: 0.232817\n",
      "Epoch: 2/2... Step: 13400... Loss: 0.292806... Val Loss: 0.221872\n",
      "Epoch: 2/2... Step: 13500... Loss: 0.158168... Val Loss: 0.231283\n",
      "Epoch: 2/2... Step: 13600... Loss: 0.163030... Val Loss: 0.238519\n",
      "Epoch: 2/2... Step: 13700... Loss: 0.172517... Val Loss: 0.231099\n",
      "Epoch: 2/2... Step: 13800... Loss: 0.217513... Val Loss: 0.224594\n",
      "Epoch: 2/2... Step: 13900... Loss: 0.249805... Val Loss: 0.225717\n",
      "Epoch: 2/2... Step: 14000... Loss: 0.239719... Val Loss: 0.223321\n",
      "Epoch: 2/2... Step: 14100... Loss: 0.299008... Val Loss: 0.233614\n",
      "Epoch: 2/2... Step: 14200... Loss: 0.210461... Val Loss: 0.230407\n",
      "Epoch: 2/2... Step: 14300... Loss: 0.196867... Val Loss: 0.222666\n",
      "Epoch: 2/2... Step: 14400... Loss: 0.296530... Val Loss: 0.221927\n",
      "Epoch: 2/2... Step: 14500... Loss: 0.179022... Val Loss: 0.229765\n",
      "Epoch: 2/2... Step: 14600... Loss: 0.242662... Val Loss: 0.228465\n",
      "Epoch: 2/2... Step: 14700... Loss: 0.463867... Val Loss: 0.228824\n",
      "Epoch: 2/2... Step: 14800... Loss: 0.230171... Val Loss: 0.225730\n",
      "Epoch: 2/2... Step: 14900... Loss: 0.336641... Val Loss: 0.231481\n",
      "Epoch: 2/2... Step: 15000... Loss: 0.282946... Val Loss: 0.226784\n",
      "Epoch: 2/2... Step: 15100... Loss: 0.238200... Val Loss: 0.224303\n",
      "Epoch: 2/2... Step: 15200... Loss: 0.224376... Val Loss: 0.225794\n",
      "Epoch: 2/2... Step: 15300... Loss: 0.179271... Val Loss: 0.225849\n",
      "Epoch: 2/2... Step: 15400... Loss: 0.259040... Val Loss: 0.239280\n",
      "Epoch: 2/2... Step: 15500... Loss: 0.318763... Val Loss: 0.234723\n",
      "Epoch: 2/2... Step: 15600... Loss: 0.260083... Val Loss: 0.231218\n",
      "Epoch: 2/2... Step: 15700... Loss: 0.313778... Val Loss: 0.227214\n",
      "Epoch: 2/2... Step: 15800... Loss: 0.175694... Val Loss: 0.232881\n",
      "Epoch: 2/2... Step: 15900... Loss: 0.246225... Val Loss: 0.226086\n",
      "Epoch: 2/2... Step: 16000... Loss: 0.215137... Val Loss: 0.234087\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    h = model.init_hidden(BATCH_SIZE)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        h = tuple([e.data for e in h])\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if counter%print_every == 0:\n",
    "            val_h = model.init_hidden(BATCH_SIZE)\n",
    "            val_losses = []\n",
    "            for inp, lab in val_loader:\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                inp, lab = inp.to(device), lab.to(device)\n",
    "                out, val_h = model(inp, val_h)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    \n",
    "    # Compute for losses\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Compute Accuracy\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    num_correct += np.sum(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.211\n",
      "Test Accuracy: 91.692%\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test Accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 200])\n"
     ]
    }
   ],
   "source": [
    "for inputs, labels in test_loader:\n",
    "    print(inputs.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    messages = text\n",
    "    for i, sentence in enumerate(messages):\n",
    "        # For test sentences, we have to tokenize the sentences as well\n",
    "        messages[i] = [word2idx[word.lower()] if word in word2idx else 0 for word in nltk.regexp_tokenize(sentence, pattern=\"\\s+\", gaps = True)]\n",
    "    messages = pad_input(messages, 200)\n",
    "    h = model.init_hidden(1)\n",
    "    for msg in messages:\n",
    "        h = tuple([each.data for each in h])\n",
    "        msg = torch.from_numpy(msg).to(device).unsqueeze(0)\n",
    "        output, h = model(msg, h)\n",
    "        pred = torch.round(output.squeeze())\n",
    "        if pred.item() == 0.0:\n",
    "            return 'Negative Review'\n",
    "        else:\n",
    "            return 'Positive Review'\n",
    "        return pred.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89330091e4604f81a05a18db47933bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Sentiment: Positive Review\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"The item was good and the seller, Kristian Espina, is also responsive to my inquiries.\"\n",
    "]\n",
    "print(f'Predicted Sentiment: {predict(messages)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb3f502ba214adfb20c47290db2c441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Sentiment: Negative Review\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "\"The shoes was good but Kristian Espina, the seller, was not responsive. The shipping time is also very slow\"\n",
    "]\n",
    "print(f'Predicted Sentiment: {predict(messages)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
